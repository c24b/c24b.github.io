{
    "docs": [
        {
            "location": "/", 
            "text": "Welcome to CortextDoc\n\n\nDocumentation langage is not yet stabilized.\nInstallation procedure is in english\nTutorial in french\n\n\nWhat is Crawtext?\n\n\nCrawtext is web-crawler that collect textual data on the web \naround a specific theme. You'll find here how to configure crawtext\nand run a crawl\n\n\nQu'est ce que Crawtext?\n\n\nCrawtext est un \ncrawler\n \nou un \nrobot d'indexation de texte\n \nqui permet la constitution de gros corpus web textuels\nissus de page web\nautour d'une expression de recherche donn\u00e9e \nde mani\u00e8re r\u00e9currente selon la fr\u00e9quence souhait\u00e9e.\n\n\nInitialement pr\u00e9vue pour \u00eatre int\u00e9gr\u00e9e \u00e0 la plateforme Cortext Manager comme un outil de constitution de datasets issu du web. \nElle fonctionne pour le moment en mode console et de mani\u00e8re ind\u00e9pendante.", 
            "title": "Home"
        }, 
        {
            "location": "/#welcome-to-cortextdoc", 
            "text": "Documentation langage is not yet stabilized.\nInstallation procedure is in english\nTutorial in french", 
            "title": "Welcome to CortextDoc"
        }, 
        {
            "location": "/#what-is-crawtext", 
            "text": "Crawtext is web-crawler that collect textual data on the web \naround a specific theme. You'll find here how to configure crawtext\nand run a crawl", 
            "title": "What is Crawtext?"
        }, 
        {
            "location": "/#quest-ce-que-crawtext", 
            "text": "Crawtext est un  crawler  \nou un  robot d'indexation de texte  \nqui permet la constitution de gros corpus web textuels\nissus de page web\nautour d'une expression de recherche donn\u00e9e \nde mani\u00e8re r\u00e9currente selon la fr\u00e9quence souhait\u00e9e.  Initialement pr\u00e9vue pour \u00eatre int\u00e9gr\u00e9e \u00e0 la plateforme Cortext Manager comme un outil de constitution de datasets issu du web. \nElle fonctionne pour le moment en mode console et de mani\u00e8re ind\u00e9pendante.", 
            "title": "Qu'est ce que Crawtext?"
        }, 
        {
            "location": "/introduction/", 
            "text": "Qu'est ce que Crawtext?\n\n\nCrawtext est un \ncrawler\n \nou un \nrobot d'indexation de texte\n \nou encore appel\u00e9 un \nmoissonneur\n en fran\u00e7ais\nC'est un robot qui permet la constitution de gros corpus web textuels\nissus de page web\nautour d'une expression de recherche donn\u00e9e \nde mani\u00e8re r\u00e9currente selon la fr\u00e9quence souhait\u00e9e.\n\n\nA quoi ca sert?\n\n\nCrawtext est un moissonneur d'informations issues des pages web. Il permet \u00e0 un utilisateur de collecter les informations contenues dans les pages web autour d'un sujet et ce sur une fr\u00e9quence r\u00e9guli\u00e8re. Il permet ainsi de collecter ainsi des informations du web (acteurs, contenus des d\u00e9bats, liens entre acteur) de mani\u00e8re historicis\u00e9e (dans son \u00e9volution) et centr\u00e9e autour d'une question.\n\n\nComment ca marche?\n\n\nCrawtext est un moissonneur du web qui suit quelques principes simples. \n\n\nIci\n pour ceux qui le souhaite, un petit rappel utile pour comprendre le fonctionnement de ce robot sur ce qu'est le web, Internet et un site web. \n\n\nA partir d'une page web de d\u00e9part appel\u00e9e \nseed\n, \nle robot crawler (qu'on compare \u00e0 une araign\u00e9e) visite la page et recup\u00e8re dans la page html tous les liens disponibles qui prennent la forme de :\n\na href=\"lien\nancre\n/a\n\n\nIl collecte et stocke les liens puis visite une \u00e0 une les pages, r\u00e9cup\u00e8re les liens, charge la page puis v\u00e9rifie que l'expression de recherche est bien dans la page, si l'expression est trouv\u00e9e il recommence le processus jusqu'\u00e0 ce que plus une page ne soit  pertinente.\nEn pseudo code simplifi\u00e9 cela donnerait\n\n\nqueue = liste des urls de d\u00e9part\ntant que la queue de traitement n'est pas vide:\n  pour chaque url:\n    crawler le contenu de l'url\n    si la requ\u00eate est pr\u00e9sente dans le contenu\n      * on stocke l'information \n      * on ajoute les nouvelles urls cit\u00e9es dans la queue de traitement\n      * on supprime l'url de la queue de traitement\n\n\n\n\n\n\nStrat\u00e9gies de crawl\n\n\nTypes de crawl\n\n\nCrawtext est un crawler web qui poss\u00e8de plusieurs types de comportement. \nqui correspondent \u00e0 plusieurs strat\u00e9gies de crawl param\u00e9trables par l'utilisateur:\n\n crawl d'un sujet/th\u00e9matique exprim\u00e9e \u00e0 travers une expression de recherche\n\n crawl d'un site web complet\n* crawl mixte sur un ou plusieurs sites web sources d'un sujet ou th\u00e9matique \nexprim\u00e9e \u00e0 travers une expression de recherche\n\n\nLe crawl n\u00e9cessite un point de d\u00e9part pour d\u00e9marrer son parcours appel\u00e9s \nseeds\n\nPlusieurs m\u00e9thodes sont propos\u00e9es qui peuvent \u00eatre mix\u00e9s en ajoutant:\n\n une url simple\n\n un fichier contenant une url par ligne\n* une cl\u00e9 d'API au moteur de recherche \nBING\n: \n    * on peut l'obtenir en s'inscrivant \nici\n\n    * une expression de recherche est alors indispensable au fonctionnement du crawl parce que les r\u00e9sultats de recherche constitue le point de d\u00e9part du crawl\n\n\nFiltres\n\n\nPlusieurs filtres additionnels sont propos\u00e9s:\n\n\n\n\n\n\nun filtre de langue: le crawler selectionnera uniquement les textes qui correspondent \u00e0 la langue selectionn\u00e9e au format \nISO 639-1\n\n\n\n\n\n\nun filtre de profondeur de crawl: le crawler arretera la recherche quand le nombre d'\u00e9tape sera atteint.\n      Ce filtre permet de r\u00e9duire \n      les temps de traitement souvent tr\u00e8s long\n      et de controler les \u00e9largissements successifs \n      du p\u00e9rim\u00eatre de recherche\n\n\n\n\n\n\nun filtre de recherche: \nle crawler selectionnera uniquement les textes qui correspondent \u00e0 cette expression de recherche.\n\n\nCe filtre est indispensable dans le cas d'un crawl autour \n  d'un sujet ou d'une th\u00e9matique\n\n\n\n\n\n\nPour plus d'information sur les filtres: voir \nConfiguration\n\n\nFr\u00e9quence\n\n\nOn peut programmer la \nr\u00e9currence\n du crawl en sp\u00e9cifiant une fr\u00e9quence:\n * journali\u00e8re\n * hebdomadaire \n * ou mensuelle\n\n\nPour plus d'information sur les filtres: voir \nConfiguration\n\n\nLimitations de Crawtext\n\n\nInterface web\n\n\nLe crawler ne poss\u00e8de pas d'interface de configuration web pour le moment\n\n\nType de donn\u00e9es collect\u00e9es\n\n\nLe crawler Crawtext ne r\u00e9cup\u00e8re pas tous les types d'information disponible sur le web. \n\n\nSont exclus (pour le moment): \n\n pdf\n\n fichiers\n\n videos\n\n sons \n\n images\n\n flash \n\n\nEn revanche, crawtext stocke \u00e0 la fois:\n\n la \npage html brute\n \n\n le \ntexte\n de la page nettoy\u00e9e\n* des informations contextuelles suppl\u00e9mentaires\n\n\nBlockage des pubs et r\u00e9seaux sociaux\n\n\nLe crawler Crawtext bloque les pages commerciales (pubs, questionnaires, pop-ups) et les r\u00e9seaux sociaux en utilisant un fichier AdBlocks qui n'est \u00e0 ce stade pas configurable et n'est pas mis \u00e0 jour automatiquement pour le moment.\nLes sites webs dont les contenus sont charg\u00e9s dynamiquement ne sont pas non plus support\u00e9s.\n\n\nTemps de traitement et capacit\u00e9 de stockage\n\n\nEn fonction du nombre d'url de d\u00e9part, de la finesse de la requete, le crawler peut mettre de quleques heures \u00e0 plusieurs jours \u00e0 compl\u00e9ter ses t\u00e2ches et les donn\u00e9es collect\u00e9es peuvent prendre \u00e9norm\u00e9ment de place.\nIl faut donc etre bien attentif \u00e0 calibrer son crawl avant de le lancer et de tester pour le calibrer.\n\n\n\n\nPour plus d'information sur l'\u00e9tat de l'art et les limitations: voir \nDevelopper Guide\n\net les choses \u00e0 faire \nTODO", 
            "title": "Introduction"
        }, 
        {
            "location": "/introduction/#quest-ce-que-crawtext", 
            "text": "Crawtext est un  crawler  \nou un  robot d'indexation de texte  \nou encore appel\u00e9 un  moissonneur  en fran\u00e7ais\nC'est un robot qui permet la constitution de gros corpus web textuels\nissus de page web\nautour d'une expression de recherche donn\u00e9e \nde mani\u00e8re r\u00e9currente selon la fr\u00e9quence souhait\u00e9e.", 
            "title": "Qu'est ce que Crawtext?"
        }, 
        {
            "location": "/introduction/#a-quoi-ca-sert", 
            "text": "Crawtext est un moissonneur d'informations issues des pages web. Il permet \u00e0 un utilisateur de collecter les informations contenues dans les pages web autour d'un sujet et ce sur une fr\u00e9quence r\u00e9guli\u00e8re. Il permet ainsi de collecter ainsi des informations du web (acteurs, contenus des d\u00e9bats, liens entre acteur) de mani\u00e8re historicis\u00e9e (dans son \u00e9volution) et centr\u00e9e autour d'une question.", 
            "title": "A quoi ca sert?"
        }, 
        {
            "location": "/introduction/#comment-ca-marche", 
            "text": "Crawtext est un moissonneur du web qui suit quelques principes simples.   Ici  pour ceux qui le souhaite, un petit rappel utile pour comprendre le fonctionnement de ce robot sur ce qu'est le web, Internet et un site web.   A partir d'une page web de d\u00e9part appel\u00e9e  seed , \nle robot crawler (qu'on compare \u00e0 une araign\u00e9e) visite la page et recup\u00e8re dans la page html tous les liens disponibles qui prennent la forme de : a href=\"lien ancre /a  Il collecte et stocke les liens puis visite une \u00e0 une les pages, r\u00e9cup\u00e8re les liens, charge la page puis v\u00e9rifie que l'expression de recherche est bien dans la page, si l'expression est trouv\u00e9e il recommence le processus jusqu'\u00e0 ce que plus une page ne soit  pertinente.\nEn pseudo code simplifi\u00e9 cela donnerait  queue = liste des urls de d\u00e9part\ntant que la queue de traitement n'est pas vide:\n  pour chaque url:\n    crawler le contenu de l'url\n    si la requ\u00eate est pr\u00e9sente dans le contenu\n      * on stocke l'information \n      * on ajoute les nouvelles urls cit\u00e9es dans la queue de traitement\n      * on supprime l'url de la queue de traitement", 
            "title": "Comment ca marche?"
        }, 
        {
            "location": "/introduction/#strategies-de-crawl", 
            "text": "", 
            "title": "Strat\u00e9gies de crawl"
        }, 
        {
            "location": "/introduction/#types-de-crawl", 
            "text": "Crawtext est un crawler web qui poss\u00e8de plusieurs types de comportement. \nqui correspondent \u00e0 plusieurs strat\u00e9gies de crawl param\u00e9trables par l'utilisateur:  crawl d'un sujet/th\u00e9matique exprim\u00e9e \u00e0 travers une expression de recherche  crawl d'un site web complet\n* crawl mixte sur un ou plusieurs sites web sources d'un sujet ou th\u00e9matique \nexprim\u00e9e \u00e0 travers une expression de recherche  Le crawl n\u00e9cessite un point de d\u00e9part pour d\u00e9marrer son parcours appel\u00e9s  seeds \nPlusieurs m\u00e9thodes sont propos\u00e9es qui peuvent \u00eatre mix\u00e9s en ajoutant:  une url simple  un fichier contenant une url par ligne\n* une cl\u00e9 d'API au moteur de recherche  BING : \n    * on peut l'obtenir en s'inscrivant  ici \n    * une expression de recherche est alors indispensable au fonctionnement du crawl parce que les r\u00e9sultats de recherche constitue le point de d\u00e9part du crawl", 
            "title": "Types de crawl"
        }, 
        {
            "location": "/introduction/#filtres", 
            "text": "Plusieurs filtres additionnels sont propos\u00e9s:    un filtre de langue: le crawler selectionnera uniquement les textes qui correspondent \u00e0 la langue selectionn\u00e9e au format  ISO 639-1    un filtre de profondeur de crawl: le crawler arretera la recherche quand le nombre d'\u00e9tape sera atteint.\n      Ce filtre permet de r\u00e9duire \n      les temps de traitement souvent tr\u00e8s long\n      et de controler les \u00e9largissements successifs \n      du p\u00e9rim\u00eatre de recherche    un filtre de recherche: \nle crawler selectionnera uniquement les textes qui correspondent \u00e0 cette expression de recherche.  Ce filtre est indispensable dans le cas d'un crawl autour \n  d'un sujet ou d'une th\u00e9matique    Pour plus d'information sur les filtres: voir  Configuration", 
            "title": "Filtres"
        }, 
        {
            "location": "/introduction/#frequence", 
            "text": "On peut programmer la  r\u00e9currence  du crawl en sp\u00e9cifiant une fr\u00e9quence:\n * journali\u00e8re\n * hebdomadaire \n * ou mensuelle  Pour plus d'information sur les filtres: voir  Configuration", 
            "title": "Fr\u00e9quence"
        }, 
        {
            "location": "/introduction/#limitations-de-crawtext", 
            "text": "", 
            "title": "Limitations de Crawtext"
        }, 
        {
            "location": "/introduction/#interface-web", 
            "text": "Le crawler ne poss\u00e8de pas d'interface de configuration web pour le moment", 
            "title": "Interface web"
        }, 
        {
            "location": "/introduction/#type-de-donnees-collectees", 
            "text": "Le crawler Crawtext ne r\u00e9cup\u00e8re pas tous les types d'information disponible sur le web.   Sont exclus (pour le moment):   pdf  fichiers  videos  sons   images  flash   En revanche, crawtext stocke \u00e0 la fois:  la  page html brute    le  texte  de la page nettoy\u00e9e\n* des informations contextuelles suppl\u00e9mentaires", 
            "title": "Type de donn\u00e9es collect\u00e9es"
        }, 
        {
            "location": "/introduction/#blockage-des-pubs-et-reseaux-sociaux", 
            "text": "Le crawler Crawtext bloque les pages commerciales (pubs, questionnaires, pop-ups) et les r\u00e9seaux sociaux en utilisant un fichier AdBlocks qui n'est \u00e0 ce stade pas configurable et n'est pas mis \u00e0 jour automatiquement pour le moment.\nLes sites webs dont les contenus sont charg\u00e9s dynamiquement ne sont pas non plus support\u00e9s.", 
            "title": "Blockage des pubs et r\u00e9seaux sociaux"
        }, 
        {
            "location": "/introduction/#temps-de-traitement-et-capacite-de-stockage", 
            "text": "En fonction du nombre d'url de d\u00e9part, de la finesse de la requete, le crawler peut mettre de quleques heures \u00e0 plusieurs jours \u00e0 compl\u00e9ter ses t\u00e2ches et les donn\u00e9es collect\u00e9es peuvent prendre \u00e9norm\u00e9ment de place.\nIl faut donc etre bien attentif \u00e0 calibrer son crawl avant de le lancer et de tester pour le calibrer.   Pour plus d'information sur l'\u00e9tat de l'art et les limitations: voir  Developper Guide \net les choses \u00e0 faire  TODO", 
            "title": "Temps de traitement et capacit\u00e9 de stockage"
        }, 
        {
            "location": "/installation/", 
            "text": "Installation\n\n\n\n\nApercu des briques logicielles\n\n\nInstaller MongoDB\n\n\nMise en place d'un virtualenv\n\n\nInstallation du parser LXML\n\n\nInstallation des d\u00e9pendances du projet\n\n\n\n\nApercu des briques logicielles\n\n\nCrawtext est \u00e9crit en Python 2.7 avec une base de donn\u00e9es Mongo 3.2\n\n\n4 \u00e9tapes d'installation:\n\n\n\n\n\n\ninstallation de la base de donn\u00e9es en Backend MongoDB \n\n\n\n\n\n\ninstallation du parser lxml:\n\n\nOn peut rencontrer certains probl\u00e8mes \u00e0 l'installation du package python ```lxml```\nque nous contournons ici\n\n\n\n\n\n\n\ncr\u00e9ation d'un environnement virtuel et installation des packages suppl\u00e9mentaire\n\n\nIl est recommand\u00e9 d'isoler l'installation de Crawtext dans un ```virtualenv```\net profiter du syst\u00e8me d'installation simplifi\u00e9e avec ```pip```\n\n\n\n\n\n\n\ncloner le repository de crawtext\n\n\nOu simplement le t\u00e9l\u00e9charger en zip\n\n\n\n\n\n\n\nFor implementation choice and the necessary what I learnt \ncf. Developper Guide\n\n\nNext steps installation are in \nEnglish\n. \nD\u00e9sol\u00e9 les gars, j'ai tout commenc\u00e9 en anglais\n\n\nInstall MongoDB\n\n\nMongo has to be install first and outside the environnement \n\n\n\n\n\n\nOn LINUX (Debian based distribution):\nPackages are compatibles with:\n\n\n\n\n\n\nDebian 7 Wheezy (and older)\n\n\n\n\n\n\nUbuntu 12.04 LTS and 14.04 LTS (and older)\n\n\n\n\n\n\n\n\n\n\nsudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv EA312927\necho \ndeb http://repo.mongodb.org/apt/debian wheezy/mongodb-org/3.2 main\n | sudo tee /etc/apt/sources.list.d/mongodb-org-3.2.list\nsudo apt-get update\nsudo apt-get install -y mongodb-org=3.2.1 mongodb-org-server=3.2.1 mongodb-org-shell=3.2.1 mongodb-org-mongos=3.2.1 mongodb-org-tools=3.2.1\n\n\n\n\n\n\nOn MAC OS/X: \n(from LionX to newest)\n\n\n\n\nbrew update\nbrew install mongodb --with-openssl\n\n\n\n\n\n\n\n\nOn Windows:\n\n\nrefer to \nofficial MongoDB installation procedure\n\n\n\n\n\n\nLet's verify now that mongo is running properly\n\n\n$ mongo\nMongoDB shell version: 3.2.0\nconnecting to: test\n\n\n\n\n\n\nType Ctrl+C to quit\n\n\nInstall LXML\n\n\nInstall LXML may cause some troubelshooting: to avoid it install \nadditionnal packages outside the environnement\n\n\n\n\nOn Debian\n\n\n\n\nsudo apt-get install libxml2-dev libxslt-dev python-dev\n\n\n\n\n\n\nOn MAC\n\n\n\n\nbrew install libxml2\nbrew install libxslt\nbrew link libxml2 --force\nbrew link libxslt --force\n\n\n\n\n\n\nOn Windows\n\n\n\n\nSelect the source file that corresponds to you architecture (32 or 64 bits)\nopen an run it\n\nlxml ditributions\n\n\nCreate a virtualenv\n\n\nVerify that virtualenv is installed\n\n\n$ virtualenv --version\n\n\n\n\nIf you got a \u201cCommand not found\u201d when you tried to use virtualenv, try:\n\n\n$ sudo pip install virtualenv\n\n\n\n\nor\n\n\nsudo apt-get install python-virtualenv # for a Debian-based system\n\n\n\n\nCreate a cortext-box and activate the virtual-env\n\n\n$ virtualenv cortext-box\n$ cd cortext-box\n$ source bin/activate\n\n\n\n\n(source bin/deactivate to exit)\n\n\nClone the repository\n\n\n$ git clone https://github.com/cortext/crawtext\n$ cd crawtext\n\n\n\n\nor simply download it\nand install additional packages using the requirements file\n\n\n$ pip install -r requirements.pip\n\n\n\n\n\nAnd that's all for now folks!\n\n\nLet's see now :\n\n\n\n\n\n\nhow to configure the crawtext \nenvironnement\n\n\n\n\n\n\nand make \nour first project", 
            "title": "Installation"
        }, 
        {
            "location": "/installation/#installation", 
            "text": "Apercu des briques logicielles  Installer MongoDB  Mise en place d'un virtualenv  Installation du parser LXML  Installation des d\u00e9pendances du projet", 
            "title": "Installation"
        }, 
        {
            "location": "/installation/#apercu-des-briques-logicielles", 
            "text": "Crawtext est \u00e9crit en Python 2.7 avec une base de donn\u00e9es Mongo 3.2  4 \u00e9tapes d'installation:    installation de la base de donn\u00e9es en Backend MongoDB     installation du parser lxml:  On peut rencontrer certains probl\u00e8mes \u00e0 l'installation du package python ```lxml```\nque nous contournons ici    cr\u00e9ation d'un environnement virtuel et installation des packages suppl\u00e9mentaire  Il est recommand\u00e9 d'isoler l'installation de Crawtext dans un ```virtualenv```\net profiter du syst\u00e8me d'installation simplifi\u00e9e avec ```pip```    cloner le repository de crawtext  Ou simplement le t\u00e9l\u00e9charger en zip    For implementation choice and the necessary what I learnt  cf. Developper Guide  Next steps installation are in  English . \nD\u00e9sol\u00e9 les gars, j'ai tout commenc\u00e9 en anglais", 
            "title": "Apercu des briques logicielles"
        }, 
        {
            "location": "/installation/#install-mongodb", 
            "text": "Mongo has to be install first and outside the environnement     On LINUX (Debian based distribution):\nPackages are compatibles with:    Debian 7 Wheezy (and older)    Ubuntu 12.04 LTS and 14.04 LTS (and older)      sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv EA312927\necho  deb http://repo.mongodb.org/apt/debian wheezy/mongodb-org/3.2 main  | sudo tee /etc/apt/sources.list.d/mongodb-org-3.2.list\nsudo apt-get update\nsudo apt-get install -y mongodb-org=3.2.1 mongodb-org-server=3.2.1 mongodb-org-shell=3.2.1 mongodb-org-mongos=3.2.1 mongodb-org-tools=3.2.1   On MAC OS/X: \n(from LionX to newest)   brew update\nbrew install mongodb --with-openssl    On Windows:  refer to  official MongoDB installation procedure    Let's verify now that mongo is running properly  $ mongo\nMongoDB shell version: 3.2.0\nconnecting to: test   Type Ctrl+C to quit", 
            "title": "Install MongoDB"
        }, 
        {
            "location": "/installation/#install-lxml", 
            "text": "Install LXML may cause some troubelshooting: to avoid it install \nadditionnal packages outside the environnement   On Debian   sudo apt-get install libxml2-dev libxslt-dev python-dev   On MAC   brew install libxml2\nbrew install libxslt\nbrew link libxml2 --force\nbrew link libxslt --force   On Windows   Select the source file that corresponds to you architecture (32 or 64 bits)\nopen an run it lxml ditributions", 
            "title": "Install LXML"
        }, 
        {
            "location": "/installation/#create-a-virtualenv", 
            "text": "Verify that virtualenv is installed  $ virtualenv --version  If you got a \u201cCommand not found\u201d when you tried to use virtualenv, try:  $ sudo pip install virtualenv  or  sudo apt-get install python-virtualenv # for a Debian-based system  Create a cortext-box and activate the virtual-env  $ virtualenv cortext-box\n$ cd cortext-box\n$ source bin/activate  (source bin/deactivate to exit)", 
            "title": "Create a virtualenv"
        }, 
        {
            "location": "/installation/#clone-the-repository", 
            "text": "$ git clone https://github.com/cortext/crawtext\n$ cd crawtext  or simply download it\nand install additional packages using the requirements file  $ pip install -r requirements.pip  And that's all for now folks!  Let's see now :    how to configure the crawtext  environnement    and make  our first project", 
            "title": "Clone the repository"
        }, 
        {
            "location": "/configuration/", 
            "text": "Configuration\n\n\nCrawtext se configure \u00e0 deux niveaux:\n\n\n\n\nEnvironnement\n\n\nProjet\n\n\n\n\nEnvironnement par d\u00e9faut\n\n\nCrawtext propose un fichier de configuration par d\u00e9faut: \nconfig/settings.json\n\n\nCe fichier d\u00e9finit l'environnement de fonctionnement du crawler:\n\n\n\n\n\n\nl'utilisateur du crawler: c'est dans un dossier \u00e0 son nom que se trouveront tous les crawls et leur r\u00e9sultats \n\n\n\n\n\n\nla base de donn\u00e9es en back-end qui g\u00e8re les param\u00eatres des crawls et leur r\u00e9currence\n\n\n\n\n\n\nl'environnement dans lequel seront stock\u00e9s tous les dossiers de tous les utilisateurs\n\n\n\n\n\n\nl'url o\u00f9 sont expos\u00e9s l'avancement du crawl et ses param\u00eatres(TO DO)\n\n\n\n\n\n\nLe fichier disponible \nici\n se pr\u00e9sente sous cette forme\n\n\n{\n    \nuser\n:{\n        \nusername\n: \nuser@cortext.net\n,\n        \npassword\n: \nkeepitsecret\n\n        },\n    \ndb\n: {\n        \nprovider\n: \nmongo\n,\n        \nhost\n: \nlocalhost\n,\n        \nport\n: 27017,\n        \npassword\n: \n,\n        \ndb_name\n: \ndemo_crawtext\n,\n        \ncollection\n: \nprojects\n\n    },\n    \nenv\n: {\n        \ndirectory\n: \n,\n        \nname\n: \ncrawtext\n\n        },\n    \nwebsite\n:{\n        \nhost\n: \nlocalhost\n,\n        \nport\n: 8080\n    }\n}\n\n\n\n\nIl suffit d'en modifier les valeurs et Crawtext met \u00e0 jour la configuration et le param\u00eatrage \u00e0 chaque lancement d'un crawl.\nIl est recommand\u00e9 pour les d\u00e9butants de ne changer que le \nusername\n\nVoir le \nfichier de configuration\n\n\nParam\u00e9trage d'un projet\n\n\nLa cr\u00e9ation ou mise \u00e0 jour d'un projet se fait via un fichier au format json de parametrages du projet\nUn example de parametrage est donn\u00e9 dans \nconfig/example.json\n\nd\u00e9finir son projet. \nLe d\u00e9tail des valeurs et leur fonctions est expliqu\u00e9 dans le tutoriel et plus en d\u00e9tail dans l'API et le glossaire\n\n\n{\n    #definir le nom du projet de crawl\n    \nname\n: \nCOP21\n,\n    #activer les filtres en mettant \nactive\n:true \n    \nfilters\n: {\n        #profondeur maximale\n        \ndepth\n:{ \n            \nactive\n: true,\n            \ndepth\n: 5\n            },\n        #filtre de langue\n        \nlang\n:{\n            \nactive\n: false,\n\n            \nlang\n: \nen\n\n            },\n        # expression de recherche\n        \nquery\n:{\n            \nactive\n: false,\n            \nquery\n: \n(COP 21) OR (COP21)\n\n            }\n    },\n    #fr\u00e9quence du crawl\n    \nscheduler\n: {\n        \nactive\n: true,\n        \ndays\n: 7\n    },\n    #point de d\u00e9part du crawl (seeds)\n    \nseeds\n: {\n        \nurl\n:{\n            \nactive\n: false,\n            \nurl\n: \nhttp://www.lefigaro.fr\n\n            },\n        \nfile\n:{\n            \nactive\n: false,\n            \nfile\n: \n./config/sources.txt\n\n            },\n        \nsearch\n: {\n            \nactive\n: true,\n            \nkey\n: \nAPIKeyGivenByBing\n,\n            \nnb\n: 100\n            }\n        }\n}", 
            "title": "Configuration"
        }, 
        {
            "location": "/configuration/#configuration", 
            "text": "Crawtext se configure \u00e0 deux niveaux:   Environnement  Projet", 
            "title": "Configuration"
        }, 
        {
            "location": "/configuration/#environnement-par-defaut", 
            "text": "Crawtext propose un fichier de configuration par d\u00e9faut:  config/settings.json  Ce fichier d\u00e9finit l'environnement de fonctionnement du crawler:    l'utilisateur du crawler: c'est dans un dossier \u00e0 son nom que se trouveront tous les crawls et leur r\u00e9sultats     la base de donn\u00e9es en back-end qui g\u00e8re les param\u00eatres des crawls et leur r\u00e9currence    l'environnement dans lequel seront stock\u00e9s tous les dossiers de tous les utilisateurs    l'url o\u00f9 sont expos\u00e9s l'avancement du crawl et ses param\u00eatres(TO DO)    Le fichier disponible  ici  se pr\u00e9sente sous cette forme  {\n     user :{\n         username :  user@cortext.net ,\n         password :  keepitsecret \n        },\n     db : {\n         provider :  mongo ,\n         host :  localhost ,\n         port : 27017,\n         password :  ,\n         db_name :  demo_crawtext ,\n         collection :  projects \n    },\n     env : {\n         directory :  ,\n         name :  crawtext \n        },\n     website :{\n         host :  localhost ,\n         port : 8080\n    }\n}  Il suffit d'en modifier les valeurs et Crawtext met \u00e0 jour la configuration et le param\u00eatrage \u00e0 chaque lancement d'un crawl.\nIl est recommand\u00e9 pour les d\u00e9butants de ne changer que le  username \nVoir le  fichier de configuration", 
            "title": "Environnement par d\u00e9faut"
        }, 
        {
            "location": "/configuration/#parametrage-dun-projet", 
            "text": "La cr\u00e9ation ou mise \u00e0 jour d'un projet se fait via un fichier au format json de parametrages du projet\nUn example de parametrage est donn\u00e9 dans  config/example.json \nd\u00e9finir son projet. \nLe d\u00e9tail des valeurs et leur fonctions est expliqu\u00e9 dans le tutoriel et plus en d\u00e9tail dans l'API et le glossaire  {\n    #definir le nom du projet de crawl\n     name :  COP21 ,\n    #activer les filtres en mettant  active :true \n     filters : {\n        #profondeur maximale\n         depth :{ \n             active : true,\n             depth : 5\n            },\n        #filtre de langue\n         lang :{\n             active : false,\n\n             lang :  en \n            },\n        # expression de recherche\n         query :{\n             active : false,\n             query :  (COP 21) OR (COP21) \n            }\n    },\n    #fr\u00e9quence du crawl\n     scheduler : {\n         active : true,\n         days : 7\n    },\n    #point de d\u00e9part du crawl (seeds)\n     seeds : {\n         url :{\n             active : false,\n             url :  http://www.lefigaro.fr \n            },\n         file :{\n             active : false,\n             file :  ./config/sources.txt \n            },\n         search : {\n             active : true,\n             key :  APIKeyGivenByBing ,\n             nb : 100\n            }\n        }\n}", 
            "title": "Param\u00e9trage d'un projet"
        }, 
        {
            "location": "/tutorial/", 
            "text": "Tutoriel\n\n\nNotre premier crawl cibl\u00e9\n\n\nIci nous allons cr\u00e9er pour l'exmple un crawl cibl\u00e9 autour \ndes prises de paroles en ligne sur la loi travail\n\n\nPour cela  nous allons cr\u00e9er le projet loi_travail\n\n\nname: \nloi_travail\n\n\n\n\n\nPour un crawl cibl\u00e9 autour d'une th\u00e9matique nous avons besoin\nd'une expression de recherche que nous devons d\u00e9finir avec soin\nelle ne doit \u00eatre ni trop large ni trop \u00e9troite pour permettre au crawler \nde collecter les informations sur la pol\u00e9miques\n\n\nNous allons modifier dans la fichier la \"query\":\n\n\n  \nquery\n:{\n\n            \nactive\n: true, \n            \nquery\n: \nloi AND (Travail OR El K?omri)\n \n            }\n\n\n\n\nPlus de d\u00e9tails sur les expressions de recherche et leur syntaxe: Glossaire\n\n\nAu vu du bruit m\u00e9diatique autour de ce sujet nous allons limiter\nla  profondeur du crawl \u00e0 3 soit le r\u00e9sultats des recherches + 2 niveaux\npour ne pas surcharger le crawler\n\n\n \ndepth\n:{\n         \nactive\n: true, \n         \ndepth\n:3\n         },     \n\n\n\n\nPlus de d\u00e9tails sur la profondeur d'uncrawl: Glossaire\n\n\nLe contenu qui nous int\u00e9resse est en fran\u00e7ais et on d\u00e9sire filtrer la langue cible\n\n\n \nlang\n:{\n         \nactive\n: true, \n         \nlang\n:\nfr\n,\n         },     \n\n\n\n\nPlus de d\u00e9tails sur les langues support\u00e9es: Glossaire\n\n\nLe crawl doit partir d'une recherche initiale sur BING\nnous allons modifier la partie seeds\n\n\n\n\najouter la cl\u00e9 d'API pour activer la recherche en ligne\n\n\nmettre le nombre de r\u00e9sultats que nous voulons r\u00e9cup\u00e9rer (50)\n\n\net d\u00e9sactiver les autres options en mettant \u00e0 false\n\n\n\n\nPlus de d\u00e9tails sur le fonctionnement de recherche sur Bing: Glossaire\n\n\nseeds\n: {\n        \nurl\n:{\n            \nactive\n: false,\n            \nurl\n: \n\n            },\n        \nfile\n:{\n            \nactive\n: false,\n            \nfile\n: \n\n            },\n        \nsearch\n: {\n            \nactive\n: true,\n            \nkey\n: \nJ8zre1019v/dIT0oXXXXXXXXX\n,\n            \nnb\n: 50\n            }\n        }\n\n\n\n\nLe crawl sera r\u00e9p\u00e9t\u00e9 toutes les semaines (exprim\u00e9es en jour)\n\n\nscheduler\n: {\n        \nactive\n: true,\n        \ndays\n: 7\n    },\n\n\n\n\nPlus de d\u00e9tails sur les routines: Glossaire\n\n\nLa configuration de otre crawl prend donc cette forme:\n\n\n{\n    \nname\n: \nloi_travail\n,\n    \nfilters\n: {\n        \ndepth\n:{ \n            \nactive\n: true,\n            \ndepth\n: 3\n            },\n        \nlang\n:{\n            \nactive\n: true,\n            \nlang\n: \nfr\n\n            },\n        \nquery\n:{\n            \nactive\n: true,\n            \nquery\n: \nLoi AND (travail OR el khomri)\n\n            }\n    },\n    \nscheduler\n: {\n        \nactive\n: true,\n        \ndays\n: 7\n    },\n    \nseeds\n: {\n        \nurl\n:{\n            \nactive\n: false,\n            \nurl\n: \n\n            },\n        \nfile\n:{\n            \nactive\n: false,\n            \nfile\n: \n\n            },\n        \nsearch\n: {\n            \nactive\n: true,\n            \nkey\n: \nMyApiSECRETKeydIT0o\n,\n            \nnb\n: 50\n            }\n        }\n}\n\n\n\n\n\nEnregistrons le au m\u00eame endroit dans un fichier final appel\u00e9 loi_travail.json \nNous allons maintenant charger la configuration \n\n\nOuvrons le terminal activons le virtual env et d\u00e9placons nous jusqu'aux fichiers sources de crawtext\n\n\n(venv) me@ordi:$~ cd crawtext\n(venv) me@ordi:$~/crawtext/ python crawtext setup --project=loi_travail\n\n\n\n\nIl suffit maintenant de le lancer en utilisant la comande start\n\n\n(venv) me@ordi:$~ \n(venv) me@ordi:$~/crawtext/ python crawtext start --project=loi_travail\n\n\n\n\nLe crawl a commenc\u00e9 et durera tant qu'il aura des pages pertinentes \u00e0 traiter", 
            "title": "Tutorial"
        }, 
        {
            "location": "/tutorial/#tutoriel", 
            "text": "", 
            "title": "Tutoriel"
        }, 
        {
            "location": "/tutorial/#notre-premier-crawl-cible", 
            "text": "Ici nous allons cr\u00e9er pour l'exmple un crawl cibl\u00e9 autour \ndes prises de paroles en ligne sur la loi travail  Pour cela  nous allons cr\u00e9er le projet loi_travail  name:  loi_travail   Pour un crawl cibl\u00e9 autour d'une th\u00e9matique nous avons besoin\nd'une expression de recherche que nous devons d\u00e9finir avec soin\nelle ne doit \u00eatre ni trop large ni trop \u00e9troite pour permettre au crawler \nde collecter les informations sur la pol\u00e9miques  Nous allons modifier dans la fichier la \"query\":     query :{\n\n             active : true, \n             query :  loi AND (Travail OR El K?omri)  \n            }  Plus de d\u00e9tails sur les expressions de recherche et leur syntaxe: Glossaire  Au vu du bruit m\u00e9diatique autour de ce sujet nous allons limiter\nla  profondeur du crawl \u00e0 3 soit le r\u00e9sultats des recherches + 2 niveaux\npour ne pas surcharger le crawler    depth :{\n          active : true, \n          depth :3\n         },       Plus de d\u00e9tails sur la profondeur d'uncrawl: Glossaire  Le contenu qui nous int\u00e9resse est en fran\u00e7ais et on d\u00e9sire filtrer la langue cible    lang :{\n          active : true, \n          lang : fr ,\n         },       Plus de d\u00e9tails sur les langues support\u00e9es: Glossaire  Le crawl doit partir d'une recherche initiale sur BING\nnous allons modifier la partie seeds   ajouter la cl\u00e9 d'API pour activer la recherche en ligne  mettre le nombre de r\u00e9sultats que nous voulons r\u00e9cup\u00e9rer (50)  et d\u00e9sactiver les autres options en mettant \u00e0 false   Plus de d\u00e9tails sur le fonctionnement de recherche sur Bing: Glossaire  seeds : {\n         url :{\n             active : false,\n             url :  \n            },\n         file :{\n             active : false,\n             file :  \n            },\n         search : {\n             active : true,\n             key :  J8zre1019v/dIT0oXXXXXXXXX ,\n             nb : 50\n            }\n        }  Le crawl sera r\u00e9p\u00e9t\u00e9 toutes les semaines (exprim\u00e9es en jour)  scheduler : {\n         active : true,\n         days : 7\n    },  Plus de d\u00e9tails sur les routines: Glossaire  La configuration de otre crawl prend donc cette forme:  {\n     name :  loi_travail ,\n     filters : {\n         depth :{ \n             active : true,\n             depth : 3\n            },\n         lang :{\n             active : true,\n             lang :  fr \n            },\n         query :{\n             active : true,\n             query :  Loi AND (travail OR el khomri) \n            }\n    },\n     scheduler : {\n         active : true,\n         days : 7\n    },\n     seeds : {\n         url :{\n             active : false,\n             url :  \n            },\n         file :{\n             active : false,\n             file :  \n            },\n         search : {\n             active : true,\n             key :  MyApiSECRETKeydIT0o ,\n             nb : 50\n            }\n        }\n}  Enregistrons le au m\u00eame endroit dans un fichier final appel\u00e9 loi_travail.json \nNous allons maintenant charger la configuration   Ouvrons le terminal activons le virtual env et d\u00e9placons nous jusqu'aux fichiers sources de crawtext  (venv) me@ordi:$~ cd crawtext\n(venv) me@ordi:$~/crawtext/ python crawtext setup --project=loi_travail  Il suffit maintenant de le lancer en utilisant la comande start  (venv) me@ordi:$~ \n(venv) me@ordi:$~/crawtext/ python crawtext start --project=loi_travail  Le crawl a commenc\u00e9 et durera tant qu'il aura des pages pertinentes \u00e0 traiter", 
            "title": "Notre premier crawl cibl\u00e9"
        }, 
        {
            "location": "/developper_guide/", 
            "text": "", 
            "title": "Developper guide"
        }, 
        {
            "location": "/todo/", 
            "text": "TODO\n\n\nNombreuses choses restent \u00e0 faire\n\n\nParallelization and optimisation\n\n\nTextual filters, article detection and extraction\n\n\nGoogle search results and not only BING\n\n\nIntegrate PDF, Videos, Images\n\n\nEase results access and stats\n\n\nWeb site to manage crawl and results", 
            "title": "Next steps developpement"
        }, 
        {
            "location": "/todo/#todo", 
            "text": "Nombreuses choses restent \u00e0 faire", 
            "title": "TODO"
        }, 
        {
            "location": "/todo/#parallelization-and-optimisation", 
            "text": "", 
            "title": "Parallelization and optimisation"
        }, 
        {
            "location": "/todo/#textual-filters-article-detection-and-extraction", 
            "text": "", 
            "title": "Textual filters, article detection and extraction"
        }, 
        {
            "location": "/todo/#google-search-results-and-not-only-bing", 
            "text": "", 
            "title": "Google search results and not only BING"
        }, 
        {
            "location": "/todo/#integrate-pdf-videos-images", 
            "text": "", 
            "title": "Integrate PDF, Videos, Images"
        }, 
        {
            "location": "/todo/#ease-results-access-and-stats", 
            "text": "", 
            "title": "Ease results access and stats"
        }, 
        {
            "location": "/todo/#web-site-to-manage-crawl-and-results", 
            "text": "", 
            "title": "Web site to manage crawl and results"
        }, 
        {
            "location": "/stateoftheart/", 
            "text": "General consideration on this projet\n\n\nNous listerons ici l'\u00e9tat de l'art, les projets similaires existants\nEt les principales difficult\u00e9es rencontr\u00e9es dans la r\u00e9alisation de ce projet\n\n\nExisting crawlers\n\n\nNombreux sont les crawlers en ligne celui diff\u00e9re dans la philosophie g\u00e9n\u00e9rale de l'outil\n\n\nWeb Page Article detection\n\n\nJ'ai test\u00e9 bon nombre des biblioth\u00e8ques existantes de detection d'article \u00e0 l'int\u00e9rieur d'une page HTML \ntoutes centr\u00e9e autour du m\u00eame algorithme:\n\n\n\n\nnewspaper - News extraction, article extraction and content curation in Python.\n\n\nhtml2text - Convert HTML to Markdown-formatted text.\n\n\npython-goose - HTML Content/Article Extractor.\n\n\npython-readability - Fast Python port of arc90's readability tool.\n\n\npython-pocket\n - Pocket API in Python\n(algo de detection d'article a \u00e9t\u00e9 retir\u00e9 depuis)\n\n\n\n\nL'algorithme de l'ensemble de ces biblioth\u00e8ques est simple \nil est fond\u00e9 sur le calcul de densit\u00e9 de texte par tag.  \n\n\nLe constat c'est que dans le cas des blogs et des page d'acceuil de site web\nles r\u00e9sultats ne sont pas concluants: \npour les blogs du type wordpress c'est toujours le bloc A propos/About qui est selectionn\u00e9.\nEn effet les r\u00e9sum\u00e9s et les liens vers Read more... fausses les r\u00e9sultats\nPour finir, on trouve mon impl\u00e9mentation de l'extraction d'article dans extractor.py compos\u00e9 de plusieurs couches:\n\n\n\n\n\n\nlxml realise un nettoyage initial du HTML les tags non desir\u00e9 \n    \nscript, noscript, iframe, form, embed, style, footer\n\n\n\n\n\n\nbeautifulsoup enl\u00e8ve les elements avec les tags non desir\u00e9 \n    \nscript, noscript, iframe, form, embed, style, footer\n\n\n\n\npuis readlibility prend le relais favorise certains block qui contient les tags au d\u00e9triment des autres\n\n\n\n\nPOSITIVE_K = [\nentry-content\n,\npost\n,\nmain\n,\ncontent\n,\ncontainer\n,\nblog\n,\narticle*\n,\npost\n,\nentry\n, \nrow\n,]\nNEGATIVE_K = [\nlike*\n,\nad*\n,\ncomment.*\n,\ncomments\n,\ncomment-body\n,\nabout\n,\naccess\n,\nnavbar\n, \n                \nnavigation\n,\nlogin\n, \nsidebar.*?\n,\nshare.*?\n,\nrelat.*?\n,\nwidget.*?\n,\nmenu\n, \nside-nav\n]\n\n\n\n\nComme on ne peut garantir la propret\u00e9 du HTML n'importe laquelle de ses m\u00e9thodes peut \u00e9chouer. \nMais il y en a toujours au moins une qui fonctionne.\n\n\nDans l'id\u00e9al, il faudrait mixer trois approches:\n\n\n\n\nl'utilisateur peut modifier la liste des tags qu'il souhaite favoriser / d\u00e9favoriser\n\n\nl'utilisateur peut modifier la liste des tags a ne pas consid\u00e9rer pour qu'ils soient pris en compte par lxml et via beautifulSoup\n\n\nun systeme d'apprentissage est greff\u00e9 \u00e0 cette extracteur qui matche meta_information avec les tags \u00e0 favoriser et detecte lequels appliquer dans quel cas\n\n\n\n\nIl existe d'autre librairies de detection d'article et qui fonctionnent par apprentissage.\ntelles que :\n\n\n\n\nscrapely - Library for extracting structured data from HTML pages. Given some example web pages and the data to be extracted, scrapely constructs a parser for all similar pages.\n\n\nlibextract - Extract data from websites.\n\n\nsumy - A module for automatic summarization of text documents and HTML pages.\n\n\n\n\nmais dans la mesure ou notre crawler ne sait pas sur quel type de document ou sur quelle plateforme, il faudrait \nadopter l'approche mixte d\u00e9clarative + apprentissage au fur et \u00e0 mesure des crawls\n\n\nDatabase Back End\n\n\nPourquoi Mongo? What went wrong....\n\n\nJ'ai fait un choix de d\u00e9part pour la librairie Mongo qui me semblait initialement adapt\u00e9e \u00e0 ma probl\u00e9matique:\n\n structuration souple\n\n facilit\u00e9 de stockage et d' export (format JSON qui s'interface tr\u00e8s bien avec python)\n\n grande disponibilit\u00e9 et bonne perf\n\n full text search et regex search natif (tr\u00e8s important)\n\n\nMongo est pr\u00e9sent\u00e9 une des bases NoSQL de r\u00e9f\u00e9rencepour du Big Data mais\nau final le bilan global de cette base de donn\u00e9es est assez negatif sinon tr\u00e8s n\u00e9gatif.\nJ'en rencontr\u00e9 en effet de gros probl\u00e8me de stockage et de performance\n\n\nPerformance, limit storage and some hugly problem of structuration\n\n\n\n\n\n\nlimite au niveau du document\n\n\n\n\ntaille limite d'enregistrement du document\n\n\nacc\u00e8s au niveau 1 seulement de l'object JSON\n\n\nacc\u00e8s au liste: premier element seulement est disponible sans agr\u00e9gation\n\n\n\n\n\n\n\n\nlimite de requetage et edition/insertion multiple/:\n\n\n\n\npour appliquer un tri \u00e0 une base de donn\u00e9es mongo il faut que celle ci ne d\u00e9passe pas X nombres d'enregistrement\n\n\npour selectionner les documents uniques en fonction d'un crit\u00e8re il est n\u00e9cessaire de faire soit un agregate soit une double requete pour acc\u00e9der aux valeurs annexes\n\n\nlorsqu'un index est cr\u00e9er pour forcer l'enregsitrement de donn\u00e9es uniques en fonction d'une cl\u00e9: l'insertion multiple est inop\u00e9rante : en cas d'entr\u00e9e dupliqu\u00e9e impossible de catcher l'erreur\n\n\n\n\n\n\n\n\nlimite de performance de la BDD:\n    En raison des temps de parsing de mon crawler, la connexion a la base n'\u00e9tait pas persistante, \n    il faut donc forcer la base \u00e0 ne pas se refermer \u00e0 chaque enregistrement\n\n\n\n\n\n\nCrawl vertical vs Crawl horizontal\n\n\nON ne peut donc pas en l'\u00e9tat avec Mongo simplement\nr\u00e9aliser le tri des urls par niveau (la liste est trop longue) en favorisant un crawl horizontal on se retrouve donc au niveau 0 \n\u00e0 savoir envisager un simple Set() dans un pickle et des algo d'optimisation de tri par profondeur et par Index\n\n\nSome solution\n\n\nEN l'\u00e9tat le texte et le HTML ne sont pas stock\u00e9 dans la BDD \nmais en dur dans le fichier de r\u00e9sultats et les documents r\u00e9f\u00e9renc\u00e9 dans la base de donn\u00e9e.\nPareil pour les versions en fonction de la date \u00e0 chaque crawl corespond une base de donn\u00e9e de r\u00e9sultat\nqui porte la date de d\u00e9but du crawl.\n\n\nSi j'avais du temps je changerai donc de SGBD en proposant 2 bases une tr\u00e8s simple en NOSQL pour la queue de traitement\net une pour le stockage final resultats avec un POSTGRES + des fichiers en bases (en effet la question critique est ici comment filtrer efficacement \nles articles pertinents sachant qu'on utilise Whoosh et que Whoosh lui m\u00eame se cr\u00e9e un index pour scannerles docs). En l'\u00e9tat le syst\u00e8me de Backend \nest donc tout pourri.\n\n\nExecution time and paralelization\n\n\nPour ce crawler, la question du temps reste porbl\u00e9matique malgr\u00e9 le fait que \nj'ai introduit dans la derni\u00e8re version  du requetage asynchrone des page HTML\n\n\nOn a deux points de traitement tr\u00e8s lourds et tr\u00e8s longs: \n\n le t\u00e9l\u00e9chargement d'une page est fonction de la disponibilit\u00e9 du serveur de cette page\n\n le parsing en raison du nettoyage et de la detection de l'article et du texte \n\n\nEnsuite on traite un nombre tr\u00e8s important de pages dont on ignore \u00e0 l'avance combien de pges \u00e0 traiter pertinente \net sur combien de niveau il est donc contraiement \u00e0 un crawler sur site tr\u00e8s difficile de pr\u00e9voir le temps de traitement\nDe plus comme je le disais pr\u00e9c\u00e9demment, le backend ne permet pas le tri des urls par niveau (la liste est trop longue)\non ne peux pas donc favoriser un crawl horizontal sur un crawl vertical en l'\u00e9tat. Le crawl  horizontal qui traite les pages \ndans l'ordre croissant de profondeur me paraissait en effet important dans la logique de la constitution du corpus  avec l'effet boule de neige\net donc les graines seeds partent d'un moteur de recherche en favorisant par la m\u00eame unenotion annexe de visibilit\u00e9.", 
            "title": "State of the art and further question"
        }, 
        {
            "location": "/stateoftheart/#general-consideration-on-this-projet", 
            "text": "Nous listerons ici l'\u00e9tat de l'art, les projets similaires existants\nEt les principales difficult\u00e9es rencontr\u00e9es dans la r\u00e9alisation de ce projet", 
            "title": "General consideration on this projet"
        }, 
        {
            "location": "/stateoftheart/#existing-crawlers", 
            "text": "Nombreux sont les crawlers en ligne celui diff\u00e9re dans la philosophie g\u00e9n\u00e9rale de l'outil", 
            "title": "Existing crawlers"
        }, 
        {
            "location": "/stateoftheart/#web-page-article-detection", 
            "text": "J'ai test\u00e9 bon nombre des biblioth\u00e8ques existantes de detection d'article \u00e0 l'int\u00e9rieur d'une page HTML \ntoutes centr\u00e9e autour du m\u00eame algorithme:   newspaper - News extraction, article extraction and content curation in Python.  html2text - Convert HTML to Markdown-formatted text.  python-goose - HTML Content/Article Extractor.  python-readability - Fast Python port of arc90's readability tool.  python-pocket  - Pocket API in Python\n(algo de detection d'article a \u00e9t\u00e9 retir\u00e9 depuis)   L'algorithme de l'ensemble de ces biblioth\u00e8ques est simple \nil est fond\u00e9 sur le calcul de densit\u00e9 de texte par tag.    Le constat c'est que dans le cas des blogs et des page d'acceuil de site web\nles r\u00e9sultats ne sont pas concluants: \npour les blogs du type wordpress c'est toujours le bloc A propos/About qui est selectionn\u00e9.\nEn effet les r\u00e9sum\u00e9s et les liens vers Read more... fausses les r\u00e9sultats\nPour finir, on trouve mon impl\u00e9mentation de l'extraction d'article dans extractor.py compos\u00e9 de plusieurs couches:    lxml realise un nettoyage initial du HTML les tags non desir\u00e9 \n     script, noscript, iframe, form, embed, style, footer    beautifulsoup enl\u00e8ve les elements avec les tags non desir\u00e9 \n     script, noscript, iframe, form, embed, style, footer   puis readlibility prend le relais favorise certains block qui contient les tags au d\u00e9triment des autres   POSITIVE_K = [ entry-content , post , main , content , container , blog , article* , post , entry ,  row ,]\nNEGATIVE_K = [ like* , ad* , comment.* , comments , comment-body , about , access , navbar , \n                 navigation , login ,  sidebar.*? , share.*? , relat.*? , widget.*? , menu ,  side-nav ]  Comme on ne peut garantir la propret\u00e9 du HTML n'importe laquelle de ses m\u00e9thodes peut \u00e9chouer. \nMais il y en a toujours au moins une qui fonctionne.  Dans l'id\u00e9al, il faudrait mixer trois approches:   l'utilisateur peut modifier la liste des tags qu'il souhaite favoriser / d\u00e9favoriser  l'utilisateur peut modifier la liste des tags a ne pas consid\u00e9rer pour qu'ils soient pris en compte par lxml et via beautifulSoup  un systeme d'apprentissage est greff\u00e9 \u00e0 cette extracteur qui matche meta_information avec les tags \u00e0 favoriser et detecte lequels appliquer dans quel cas   Il existe d'autre librairies de detection d'article et qui fonctionnent par apprentissage.\ntelles que :   scrapely - Library for extracting structured data from HTML pages. Given some example web pages and the data to be extracted, scrapely constructs a parser for all similar pages.  libextract - Extract data from websites.  sumy - A module for automatic summarization of text documents and HTML pages.   mais dans la mesure ou notre crawler ne sait pas sur quel type de document ou sur quelle plateforme, il faudrait \nadopter l'approche mixte d\u00e9clarative + apprentissage au fur et \u00e0 mesure des crawls", 
            "title": "Web Page Article detection"
        }, 
        {
            "location": "/stateoftheart/#database-back-end", 
            "text": "", 
            "title": "Database Back End"
        }, 
        {
            "location": "/stateoftheart/#pourquoi-mongo-what-went-wrong", 
            "text": "J'ai fait un choix de d\u00e9part pour la librairie Mongo qui me semblait initialement adapt\u00e9e \u00e0 ma probl\u00e9matique:  structuration souple  facilit\u00e9 de stockage et d' export (format JSON qui s'interface tr\u00e8s bien avec python)  grande disponibilit\u00e9 et bonne perf  full text search et regex search natif (tr\u00e8s important)  Mongo est pr\u00e9sent\u00e9 une des bases NoSQL de r\u00e9f\u00e9rencepour du Big Data mais\nau final le bilan global de cette base de donn\u00e9es est assez negatif sinon tr\u00e8s n\u00e9gatif.\nJ'en rencontr\u00e9 en effet de gros probl\u00e8me de stockage et de performance", 
            "title": "Pourquoi Mongo? What went wrong...."
        }, 
        {
            "location": "/stateoftheart/#performance-limit-storage-and-some-hugly-problem-of-structuration", 
            "text": "limite au niveau du document   taille limite d'enregistrement du document  acc\u00e8s au niveau 1 seulement de l'object JSON  acc\u00e8s au liste: premier element seulement est disponible sans agr\u00e9gation     limite de requetage et edition/insertion multiple/:   pour appliquer un tri \u00e0 une base de donn\u00e9es mongo il faut que celle ci ne d\u00e9passe pas X nombres d'enregistrement  pour selectionner les documents uniques en fonction d'un crit\u00e8re il est n\u00e9cessaire de faire soit un agregate soit une double requete pour acc\u00e9der aux valeurs annexes  lorsqu'un index est cr\u00e9er pour forcer l'enregsitrement de donn\u00e9es uniques en fonction d'une cl\u00e9: l'insertion multiple est inop\u00e9rante : en cas d'entr\u00e9e dupliqu\u00e9e impossible de catcher l'erreur     limite de performance de la BDD:\n    En raison des temps de parsing de mon crawler, la connexion a la base n'\u00e9tait pas persistante, \n    il faut donc forcer la base \u00e0 ne pas se refermer \u00e0 chaque enregistrement", 
            "title": "Performance, limit storage and some hugly problem of structuration"
        }, 
        {
            "location": "/stateoftheart/#crawl-vertical-vs-crawl-horizontal", 
            "text": "ON ne peut donc pas en l'\u00e9tat avec Mongo simplement\nr\u00e9aliser le tri des urls par niveau (la liste est trop longue) en favorisant un crawl horizontal on se retrouve donc au niveau 0 \n\u00e0 savoir envisager un simple Set() dans un pickle et des algo d'optimisation de tri par profondeur et par Index", 
            "title": "Crawl vertical vs Crawl horizontal"
        }, 
        {
            "location": "/stateoftheart/#some-solution", 
            "text": "EN l'\u00e9tat le texte et le HTML ne sont pas stock\u00e9 dans la BDD \nmais en dur dans le fichier de r\u00e9sultats et les documents r\u00e9f\u00e9renc\u00e9 dans la base de donn\u00e9e.\nPareil pour les versions en fonction de la date \u00e0 chaque crawl corespond une base de donn\u00e9e de r\u00e9sultat\nqui porte la date de d\u00e9but du crawl.  Si j'avais du temps je changerai donc de SGBD en proposant 2 bases une tr\u00e8s simple en NOSQL pour la queue de traitement\net une pour le stockage final resultats avec un POSTGRES + des fichiers en bases (en effet la question critique est ici comment filtrer efficacement \nles articles pertinents sachant qu'on utilise Whoosh et que Whoosh lui m\u00eame se cr\u00e9e un index pour scannerles docs). En l'\u00e9tat le syst\u00e8me de Backend \nest donc tout pourri.", 
            "title": "Some solution"
        }, 
        {
            "location": "/stateoftheart/#execution-time-and-paralelization", 
            "text": "Pour ce crawler, la question du temps reste porbl\u00e9matique malgr\u00e9 le fait que \nj'ai introduit dans la derni\u00e8re version  du requetage asynchrone des page HTML  On a deux points de traitement tr\u00e8s lourds et tr\u00e8s longs:   le t\u00e9l\u00e9chargement d'une page est fonction de la disponibilit\u00e9 du serveur de cette page  le parsing en raison du nettoyage et de la detection de l'article et du texte   Ensuite on traite un nombre tr\u00e8s important de pages dont on ignore \u00e0 l'avance combien de pges \u00e0 traiter pertinente \net sur combien de niveau il est donc contraiement \u00e0 un crawler sur site tr\u00e8s difficile de pr\u00e9voir le temps de traitement\nDe plus comme je le disais pr\u00e9c\u00e9demment, le backend ne permet pas le tri des urls par niveau (la liste est trop longue)\non ne peux pas donc favoriser un crawl horizontal sur un crawl vertical en l'\u00e9tat. Le crawl  horizontal qui traite les pages \ndans l'ordre croissant de profondeur me paraissait en effet important dans la logique de la constitution du corpus  avec l'effet boule de neige\net donc les graines seeds partent d'un moteur de recherche en favorisant par la m\u00eame unenotion annexe de visibilit\u00e9.", 
            "title": "Execution time and paralelization"
        }
    ]
}