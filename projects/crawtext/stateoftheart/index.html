<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  
  <title>State of the art and further question - Crawtext documentation</title>
  

  <link rel="shortcut icon" href="../img/favicon.ico">

  
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="../css/highlight.css">

  
  <script>
    // Current page data
    var mkdocs_page_name = "State of the art and further question";
    var mkdocs_page_input_path = "stateoftheart.md";
    var mkdocs_page_url = "/stateoftheart/";
  </script>
  
  <script src="../js/jquery-2.1.1.min.js"></script>
  <script src="../js/modernizr-2.8.3.min.js"></script>
  <script type="text/javascript" src="../js/highlight.pack.js"></script>
  <script src="../js/theme.js"></script> 

  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href=".." class="icon icon-home"> Crawtext documentation</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
        <ul class="current">
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="..">Home</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../introduction/">Introduction</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../installation/">Installation</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../configuration/">Configuration</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../tutorial/">Tutorial</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../developper_guide/">Developper guide</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../todo/">Next steps developpement</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 current">
        <a class="current" href="./">State of the art and further question</a>
        
            <ul>
            
                <li class="toctree-l3"><a href="#general-consideration-on-this-projet">General consideration on this projet</a></li>
                
                    <li><a class="toctree-l4" href="#existing-crawlers">Existing crawlers</a></li>
                
                    <li><a class="toctree-l4" href="#web-page-article-detection">Web Page Article detection</a></li>
                
                    <li><a class="toctree-l4" href="#database-back-end">Database Back End</a></li>
                
                    <li><a class="toctree-l4" href="#execution-time-and-paralelization">Execution time and paralelization</a></li>
                
            
            </ul>
        
    </li>
<li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="..">Crawtext documentation</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="..">Docs</a> &raquo;</li>
    
      
    
    <li>State of the art and further question</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="general-consideration-on-this-projet">General consideration on this projet</h1>
<p>Nous listerons ici l'état de l'art, les projets similaires existants
Et les principales difficultées rencontrées dans la réalisation de ce projet</p>
<h2 id="existing-crawlers">Existing crawlers</h2>
<p>Nombreux sont les crawlers en ligne celui différe dans la philosophie générale de l'outil</p>
<h2 id="web-page-article-detection">Web Page Article detection</h2>
<p>J'ai testé bon nombre des bibliothèques existantes de detection d'article à l'intérieur d'une page HTML 
toutes centrée autour du même algorithme:</p>
<ul>
<li>newspaper - News extraction, article extraction and content curation in Python.</li>
<li>html2text - Convert HTML to Markdown-formatted text.</li>
<li>python-goose - HTML Content/Article Extractor.</li>
<li>python-readability - Fast Python port of arc90's readability tool.</li>
<li><a href="https://github.com/tapanpandita/pocket">python-pocket</a> - Pocket API in Python
(algo de detection d'article a été retiré depuis)</li>
</ul>
<p>L'algorithme de l'ensemble de ces bibliothèques est simple 
il est fondé sur le calcul de densité de texte par tag.  </p>
<p>Le constat c'est que dans le cas des blogs et des page d'acceuil de site web
les résultats ne sont pas concluants: 
pour les blogs du type wordpress c'est toujours le bloc A propos/About qui est selectionné.
En effet les résumés et les liens vers Read more... fausses les résultats
Pour finir, on trouve mon implémentation de l'extraction d'article dans extractor.py composé de plusieurs couches:</p>
<ul>
<li>
<p>lxml realise un nettoyage initial du HTML les tags non desiré 
    <code>script, noscript, iframe, form, embed, style, footer</code></p>
</li>
<li>
<p>beautifulsoup enlève les elements avec les tags non desiré 
    <code>script, noscript, iframe, form, embed, style, footer</code></p>
</li>
<li>puis readlibility prend le relais favorise certains block qui contient les tags au détriment des autres</li>
</ul>
<pre><code>POSITIVE_K = [&quot;entry-content&quot;,&quot;post&quot;,&quot;main&quot;,&quot;content&quot;,&quot;container&quot;,&quot;blog&quot;,&quot;article*&quot;,&quot;post&quot;,&quot;entry&quot;, &quot;row&quot;,]
NEGATIVE_K = [&quot;like*&quot;,&quot;ad*&quot;,&quot;comment.*&quot;,&quot;comments&quot;,&quot;comment-body&quot;,&quot;about&quot;,&quot;access&quot;,&quot;navbar&quot;, 
                &quot;navigation&quot;,&quot;login&quot;, &quot;sidebar.*?&quot;,&quot;share.*?&quot;,&quot;relat.*?&quot;,&quot;widget.*?&quot;,&quot;menu&quot;, &quot;side-nav&quot;]
</code></pre>

<p>Comme on ne peut garantir la propreté du HTML n'importe laquelle de ses méthodes peut échouer. 
Mais il y en a toujours au moins une qui fonctionne.</p>
<p>Dans l'idéal, il faudrait mixer trois approches:</p>
<ul>
<li>l'utilisateur peut modifier la liste des tags qu'il souhaite favoriser / défavoriser</li>
<li>l'utilisateur peut modifier la liste des tags a ne pas considérer pour qu'ils soient pris en compte par lxml et via beautifulSoup</li>
<li>un systeme d'apprentissage est greffé à cette extracteur qui matche meta_information avec les tags à favoriser et detecte lequels appliquer dans quel cas</li>
</ul>
<p>Il existe d'autre librairies de detection d'article et qui fonctionnent par apprentissage.
telles que :</p>
<ul>
<li>scrapely - Library for extracting structured data from HTML pages. Given some example web pages and the data to be extracted, scrapely constructs a parser for all similar pages.</li>
<li>libextract - Extract data from websites.</li>
<li>sumy - A module for automatic summarization of text documents and HTML pages.</li>
</ul>
<p>mais dans la mesure ou notre crawler ne sait pas sur quel type de document ou sur quelle plateforme, il faudrait 
adopter l'approche mixte déclarative + apprentissage au fur et à mesure des crawls</p>
<h2 id="database-back-end">Database Back End</h2>
<h3 id="pourquoi-mongo-what-went-wrong">Pourquoi Mongo? What went wrong....</h3>
<p>J'ai fait un choix de départ pour la librairie Mongo qui me semblait initialement adaptée à ma problématique:
<em> structuration souple
</em> facilité de stockage et d' export (format JSON qui s'interface très bien avec python)
<em> grande disponibilité et bonne perf
</em> full text search et regex search natif (très important)</p>
<p>Mongo est présenté une des bases NoSQL de référencepour du Big Data mais
au final le bilan global de cette base de données est assez negatif sinon très négatif.
J'en rencontré en effet de gros problème de stockage et de performance</p>
<h3 id="performance-limit-storage-and-some-hugly-problem-of-structuration">Performance, limit storage and some hugly problem of structuration</h3>
<ul>
<li>
<p>limite au niveau du document</p>
<ul>
<li>taille limite d'enregistrement du document</li>
<li>accès au niveau 1 seulement de l'object JSON</li>
<li>accès au liste: premier element seulement est disponible sans agrégation</li>
</ul>
</li>
<li>
<p>limite de requetage et edition/insertion multiple/:</p>
<ul>
<li>pour appliquer un tri à une base de données mongo il faut que celle ci ne dépasse pas X nombres d'enregistrement</li>
<li>pour selectionner les documents uniques en fonction d'un critère il est nécessaire de faire soit un agregate soit une double requete pour accéder aux valeurs annexes</li>
<li>lorsqu'un index est créer pour forcer l'enregsitrement de données uniques en fonction d'une clé: l'insertion multiple est inopérante : en cas d'entrée dupliquée impossible de catcher l'erreur</li>
</ul>
</li>
<li>
<p>limite de performance de la BDD:
    En raison des temps de parsing de mon crawler, la connexion a la base n'était pas persistante, 
    il faut donc forcer la base à ne pas se refermer à chaque enregistrement</p>
</li>
</ul>
<h3 id="crawl-vertical-vs-crawl-horizontal">Crawl vertical vs Crawl horizontal</h3>
<p>ON ne peut donc pas en l'état avec Mongo simplement
réaliser le tri des urls par niveau (la liste est trop longue) en favorisant un crawl horizontal on se retrouve donc au niveau 0 
à savoir envisager un simple Set() dans un pickle et des algo d'optimisation de tri par profondeur et par Index</p>
<h3 id="some-solution">Some solution</h3>
<p>EN l'état le texte et le HTML ne sont pas stocké dans la BDD 
mais en dur dans le fichier de résultats et les documents référencé dans la base de donnée.
Pareil pour les versions en fonction de la date à chaque crawl corespond une base de donnée de résultat
qui porte la date de début du crawl.</p>
<p>Si j'avais du temps je changerai donc de SGBD en proposant 2 bases une très simple en NOSQL pour la queue de traitement
et une pour le stockage final resultats avec un POSTGRES + des fichiers en bases (en effet la question critique est ici comment filtrer efficacement 
les articles pertinents sachant qu'on utilise Whoosh et que Whoosh lui même se crée un index pour scannerles docs). En l'état le système de Backend 
est donc tout pourri.</p>
<h2 id="execution-time-and-paralelization">Execution time and paralelization</h2>
<p>Pour ce crawler, la question du temps reste porblématique malgré le fait que 
j'ai introduit dans la dernière version  du requetage asynchrone des page HTML</p>
<p>On a deux points de traitement très lourds et très longs: 
<em> le téléchargement d'une page est fonction de la disponibilité du serveur de cette page
</em> le parsing en raison du nettoyage et de la detection de l'article et du texte </p>
<p>Ensuite on traite un nombre très important de pages dont on ignore à l'avance combien de pges à traiter pertinente 
et sur combien de niveau il est donc contraiement à un crawler sur site très difficile de prévoir le temps de traitement
De plus comme je le disais précédemment, le backend ne permet pas le tri des urls par niveau (la liste est trop longue)
on ne peux pas donc favoriser un crawl horizontal sur un crawl vertical en l'état. Le crawl  horizontal qui traite les pages 
dans l'ordre croissant de profondeur me paraissait en effet important dans la logique de la constitution du corpus  avec l'effet boule de neige
et donc les graines seeds partent d'un moteur de recherche en favorisant par la même unenotion annexe de visibilité.</p>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
      
        <a href="../todo/" class="btn btn-neutral" title="Next steps developpement"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
	  
        </div>
      </div>

    </section>

  </div>

<div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href="../todo/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
    </span>
</div>

</body>
</html>
